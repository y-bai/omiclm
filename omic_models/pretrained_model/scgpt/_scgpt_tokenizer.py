#!/usr/bin/env python
# -*-coding:utf-8 -*-
"""
@File    :		_scgpt_tokenizer.py
@Time    :   	2024/05/29 08:59:17
@Author  :   	Yong Bai 
@Contact :   	baiyong at genomics.cn
@License :   	(C)Copyright 2023-2024, Yong Bai

                Licensed under the Apache License, Version 2.0 (the "License");
                you may not use this file except in compliance with the License.
                You may obtain a copy of the License at

                    http://www.apache.org/licenses/LICENSE-2.0

                Unless required by applicable law or agreed to in writing, software
                distributed under the License is distributed on an "AS IS" BASIS,
                WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
                See the License for the specific language governing permissions and
                limitations under the License.

@Desc    :   	None

"""

import os
import logging
from shutil import copyfile
from typing import Dict, List, Optional, Tuple

from scipy.sparse import issparse
import numpy as np
from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer

from anndata import AnnData

from scgpt.tokenizer import tokenize_and_pad_batch, tokenize_batch
from scgpt.tokenizer.gene_tokenizer import GeneVocab

logger = logging.getLogger(__name__)

VOCAB_FILES_NAMES = {
    "vocab_file": "vocab.json",
}

class ScGPTTokenizerWrapper(PreTrainedTokenizer):
    
    vocab_files_names = VOCAB_FILES_NAMES

    def __init__(
        self,
        vocab_file: str,
        cls_token: str = "<cls>",
        eos_token: str = "<eoc>",
        pad_token: str = "<pad>",
        pad_value: int = -2,
        model_max_length: int = 1200,
        **kwargs,
    ) -> None:
        
        self.vocab_file = vocab_file
        self.model_max_length = model_max_length

        cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token
        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token
        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token

        self._vocab2id = GeneVocab.from_file(vocab_file)
        self._vocab2id.set_default_index(self._vocab2id["<pad>"])

        self.pad_value = pad_value

        super().__init__(
            cls_token=cls_token,
            eos_token=eos_token,
            pad_token=pad_token,
            model_max_length = model_max_length,
            pad_value=pad_value,
            add_prefix_space=False,
            **kwargs,
        )
    
    @property
    def vocab_size(self):
        return len(self._vocab2id)
    
    def get_vocab(self) -> Dict[str, int]:
        return self._vocab2id.get_stoi()
    
    def _convert_token_to_id(self, token):
        return self._vocab2id.lookup_indices([token])[0]
       
    def _convert_id_to_token(self, index:int):
        return self._vocab2id.lookup_tokens([index])[0]
    
    def __call__(
        self, 
        adata: AnnData, 
        input_layer_key: str = "X_binned",
        include_zero_genes: bool = False,
        gene_col: str = "gene_col",
        padding: bool = True,
        return_pt: bool = False, 
        **kwargs
    ):
        """ tokenize scRNA using scGPT

        Parameters
        ----------
        adata : AnnData
            adata preprocessed by `data_prep.ScGPTDataProcessor`
        input_layer_key : str, optional
            input layer key generated by `data_prep.ScGPTDataProcessor`, by default "X_binned"
        include_zero_genes : bool, optional
            by default False
        gene_col : str, optional
            gene_col in preprocessed adata.var, by default "gene_col"
        return_pt : bool, optional
            whether returning pytorch tensor, by default False

        Returns
        -------
        _type_
            _description_

        Raises
        ------
        ValueError
            _description_
        """
        if input_layer_key not in adata.layers.keys():
            raise ValueError(f"{input_layer_key} is not in adata.layers, Using `data_prep.ScGPTDataProcessor` to preprocess data first")
        
        input_data = (
            adata.layers[input_layer_key].A
            if issparse(adata.layers[input_layer_key])
            else adata.layers[input_layer_key]
        )

        genes = adata.var[gene_col].tolist()
        gene_ids = np.array(self._vocab2id(genes), dtype=int)

        if padding:
            tokenized_data = tokenize_and_pad_batch(
                input_data,
                gene_ids,
                max_len = self.model_max_length,
                vocab = self._vocab2id,
                pad_token = self.pad_token if isinstance(self.pad_token, str) else str(self.pad_token),
                pad_value = self.pad_value,
                # append <cls> token at the beginning
                append_cls = self.cls_token if isinstance(self.cls_token, str) else str(self.cls_token),  
                include_zero_gene = include_zero_genes,
                return_pt = return_pt,
            )
        else:
            tokenized_data = tokenize_batch(
                input_data,
                gene_ids,
                return_pt=return_pt,
                append_cls=self.cls_token if isinstance(self.cls_token, str) else str(self.cls_token), 
                include_zero_gene = include_zero_genes,
                cls_id = self.cls_token_id,
                cls_id_mod_type=None,
            )
        # tokenized_data["values"] = tokenized_data["values"].float()

        return tokenized_data
    

    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        if not self.can_save_slow_tokenizer:
            raise ValueError(
                "Your fast tokenizer does not have the necessary information to save the vocabulary for a slow "
                "tokenizer."
            )

        if not os.path.isdir(save_directory):
            logger.error(f"Vocabulary path ({save_directory}) should be a directory")
            return
        out_vocab_file = os.path.join(
            save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
        )

        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):
            copyfile(self.vocab_file, out_vocab_file)

        return (out_vocab_file,)

