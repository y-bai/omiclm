cyclone001-agent-130
07/18/2024 15:41:53 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
07/18/2024 15:41:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
07/18/2024 15:41:53 - INFO - __main__ - >>> loading TOKENIZED SEQ data and split from /home/share/huadjyin/home/weiyilin/project/DNALLM/datasets/tokenized_datasets/seq_dataset/hyenadna/Switched_Bm_IGHDneg
07/18/2024 15:43:14 - INFO - __main__ - >>> spliting TRAIN DATASET into train (99.95%) and valiadtion (0.05%) datasets.
07/18/2024 15:43:14 - INFO - datasets.arrow_dataset - Loading cached split indices for dataset at /home/share/huadjyin/home/weiyilin/project/DNALLM/datasets/tokenized_datasets/seq_dataset/hyenadna/Switched_Bm_IGHDneg/train/cache-6994b8f3e2e0c140.arrow and /home/share/huadjyin/home/weiyilin/project/DNALLM/datasets/tokenized_datasets/seq_dataset/hyenadna/Switched_Bm_IGHDneg/train/cache-254c2e87bb376e67.arrow
07/18/2024 15:43:17 - INFO - __main__ - train dataset: 
Dataset({
    features: ['sample', 'pos', 'peak_value', 'sum_peak_value', 'record_id', 'input_ids', 'norm_peak_value', 'log1p_norm_peak_value'],
    num_rows: 121558634
})
07/18/2024 15:43:17 - INFO - __main__ - validation dataset: 
Dataset({
    features: ['sample', 'pos', 'peak_value', 'sum_peak_value', 'record_id', 'input_ids', 'norm_peak_value', 'log1p_norm_peak_value'],
    num_rows: 60810
})
07/18/2024 15:43:18 - INFO - __main__ - >>> loading EVAL DATASET
07/18/2024 15:43:18 - INFO - __main__ - >>> loading SCRNA EMBEDDING data (h5ad) from /home/share/huadjyin/home/weiyilin/project/DNALLM/datasets/embedding_datasets/scrna_dataset/scgpt/Switched_Bm_IGHDneg/Switched_Bm_IGHDneg_embedding.h5ad
07/18/2024 15:43:19 - INFO - __main__ - >>> LOADED pretrained model and tokenizer from /home/share/huadjyin/home/weiyilin/project/DNALLM/HyenaDNA/hyenadna-medium-450k-seqlen
07/18/2024 15:43:21 - INFO - __main__ - >>> OmicFormerConfig: 
{
  "architectures": [
    "OmicFormer"
  ],
  "dropout": 0.1,
  "embeding_l2norm": true,
  "ffn_type": "gated_mlp",
  "fusion_type": "cross_attn",
  "hidden_dim": 512,
  "initializer_range": 0.02,
  "intermediate_hidden_dim": 768,
  "moe_topk": 2,
  "n_heads": 8,
  "n_layers_encoder": 4,
  "n_layers_fusion": 8,
  "n_outputs": 1,
  "num_experts": 4,
  "out_pooling_mode": "adaptive",
  "out_pooling_size": 3,
  "pre_layer_type": "gated_mlp",
  "seq_input_pooling_size": 501
}
07/18/2024 15:43:21 - INFO - __main__ - >>> OmicFormerPreTrainedModel: 
OmicFormerPreTrainedModel(
  (seq_emb_model): HyenaDNAPreTrainedModel(
    (model): HyenaDNAModel(
      (backbone): LMBackbone(
        (embeddings): GPT2Embeddings(
          (word_embeddings): Embedding(16, 256)
        )
        (layers): ModuleList(
          (0): Block(
            (mixer): HyenaOperator(
              (dropout): Dropout(p=0.0, inplace=False)
              (in_proj): Linear(in_features=256, out_features=768, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)
              (filter_fn): HyenaFilter(
                (dropout): Dropout(p=0.0, inplace=False)
                (pos_emb): PositionalEmbedding()
                (implicit_filter): Sequential(
                  (0): Linear(in_features=5, out_features=64, bias=True)
                  (1): Sin()
                  (2): Linear(in_features=64, out_features=64, bias=True)
                  (3): Sin()
                  (4): Linear(in_features=64, out_features=64, bias=True)
                  (5): Sin()
                  (6): Linear(in_features=64, out_features=256, bias=False)
                )
                (modulation): ExponentialModulation()
              )
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (drop_path1): StochasticDepth(p=0.0, mode=row)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=256, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (drop_path2): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1-7): 7 x Block(
            (mixer): HyenaOperator(
              (dropout): Dropout(p=0.0, inplace=False)
              (in_proj): Linear(in_features=256, out_features=768, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)
              (filter_fn): HyenaFilter(
                (dropout): Dropout(p=0.0, inplace=False)
                (pos_emb): PositionalEmbedding()
                (implicit_filter): Sequential(
                  (0): Linear(in_features=5, out_features=64, bias=True)
                  (1): Sin()
                  (2): Linear(in_features=64, out_features=64, bias=True)
                  (3): Sin()
                  (4): Linear(in_features=64, out_features=64, bias=True)
                  (5): Sin()
                  (6): Linear(in_features=64, out_features=256, bias=False)
                )
                (modulation): ExponentialModulation()
              )
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (drop_path1): StochasticDepth(p=0.0, mode=row)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=256, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (drop_path2): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (drop_f): Dropout(p=0.0, inplace=False)
        (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (model): OmicFormer(
    (seq_input_proj): OmicInputProjection(
      (pre_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): GatedMLP(
          (fc1): Linear(in_features=256, out_features=1536, bias=True)
          (fc2): Linear(in_features=768, out_features=512, bias=True)
        )
      )
    )
    (scrna_input_proj): OmicInputProjection(
      (pre_proj): Sequential(
        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (1): GatedMLP(
          (fc1): Linear(in_features=512, out_features=1536, bias=True)
          (fc2): Linear(in_features=768, out_features=512, bias=True)
        )
      )
    )
    (seq_emb_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (scrna_emb_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (seq_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-3): 4 x EncoderBlock(
          (self_attn): MHA(
            (Wqkv): Linear(in_features=512, out_features=1536, bias=True)
            (dwconv_qkv): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)
            (inner_attn): SelfAttention(
              (drop): Dropout(p=0.1, inplace=False)
            )
            (inner_cross_attn): CrossAttention(
              (drop): Dropout(p=0.1, inplace=False)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (ffn): GatedMLP(
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=768, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (scrna_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-3): 4 x EncoderBlock(
          (self_attn): MHA(
            (Wqkv): Linear(in_features=512, out_features=1536, bias=True)
            (dwconv_qkv): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)
            (inner_attn): SelfAttention(
              (drop): Dropout(p=0.1, inplace=False)
            )
            (inner_cross_attn): CrossAttention(
              (drop): Dropout(p=0.1, inplace=False)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (ffn): GatedMLP(
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=768, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (fusion): TransformerEncoder(
      (layers): ModuleList(
        (0-7): 8 x EncoderBlock(
          (self_attn): MHA(
            (Wq): Linear(in_features=512, out_features=512, bias=True)
            (Wkv): Linear(in_features=512, out_features=1024, bias=True)
            (inner_attn): SelfAttention(
              (drop): Dropout(p=0.1, inplace=False)
            )
            (inner_cross_attn): CrossAttention(
              (drop): Dropout(p=0.1, inplace=False)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (ffn): GatedMLP(
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=768, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (out_pooling): EmbeddingPooling(
      (ada_pool): AdaptiveAvgPool1d(output_size=3)
    )
    (out_proj): OutputLayer(
      (out): Sequential(
        (0): Linear(in_features=1536, out_features=768, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=1, bias=True)
      )
    )
  )
)
07/18/2024 15:43:21 - INFO - __main__ - num params: 45507585
07/18/2024 15:43:21 - INFO - __main__ - num trainable params: 38957057
07/18/2024 15:43:21 - INFO - __main__ - ^^^^^^^^tf32 is set: True
07/18/2024 15:43:21 - INFO - __main__ - ^^^^^^^^fp16 = False
07/18/2024 15:43:21 - INFO - __main__ - ^^^^^^^^Learning rate: 0.0006
07/18/2024 15:43:21 - INFO - __main__ - ^^^^^^^^LR scheduler type : cosine
07/18/2024 15:43:21 - INFO - __main__ - ^^^^^^^^use streaming : False
07/18/2024 15:43:21 - WARNING - accelerate.utils.other - Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
07/18/2024 15:43:22 - INFO - __main__ - >>>>>>>>>>>>>>>>Start training and evaluatoin......
{'loss': 0.9225, 'grad_norm': 0.16115646064281464, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.0005923061796488335}
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 71
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72batch size: 71

batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 71
batch size: 72
batch size: 72batch size: 72

batch size: 72batch size: 71

batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72batch size: 71

batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72batch size: 72

batch size: 72batch size: 72

batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 71
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 72
batch size: 71
batch size: 70batch size: 72

